{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMG6FgOGbcGg"
      },
      "outputs": [],
      "source": [
        "!pip install transformers timm fairscale datasets\n",
        "!git clone https://github.com/salesforce/BLIP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "IkT6HMFCbk3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_dataset_folder = \"./historic_img/\"\n",
        "name_for_dataset = \"historic_images\""
      ],
      "metadata": {
        "id": "WuzWW07uNJV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/BLIP\n",
        "!mkdir $local_dataset_folder"
      ],
      "metadata": {
        "id": "wVMNt18abvLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDTgYGVIbcGh"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import Dataset\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from models.blip import blip_decoder\n",
        "\n",
        "torch_device = None\n",
        "transform = None\n",
        "model = None\n",
        "data = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x4unEvhbcGj"
      },
      "outputs": [],
      "source": [
        "def setup(image_size):\n",
        "    global torch_device\n",
        "    global transform\n",
        "    global model\n",
        "\n",
        "    torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n",
        "    model.eval()\n",
        "    model = model.to(torch_device)\n",
        "\n",
        "def load_image_for_blip_inference(img_path):\n",
        "    raw_image = Image.open(img_path).convert('RGB')\n",
        "    image = transform(raw_image).unsqueeze(0).to(torch_device)\n",
        "    return image\n",
        "\n",
        "def blip_caption_for_image(img_path):\n",
        "    image = load_image_for_blip_inference(img_path)\n",
        "    caption = [\"\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        caption = model.generate(image, sample=True, top_p=0.9, max_length=64, min_length=5)\n",
        "        #print('caption: '+caption[0])\n",
        "    return caption[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5RlvoIhbcGk"
      },
      "outputs": [],
      "source": [
        "def process_image_folder(folder_name, dataset_name):\n",
        "    global data\n",
        "\n",
        "    files = list(Path(folder_name).rglob(\"*.jpg\"))\n",
        "    data = defaultdict(list)\n",
        "\n",
        "    for file in tqdm(files):\n",
        "        image = Image.open(str(file))\n",
        "        data['image'].append(image)\n",
        "        text = blip_caption_for_image(file)\n",
        "        data['text'].append(text)\n",
        "\n",
        "    dataset = Dataset.from_dict(data)\n",
        "\n",
        "    print(\"Dataset was generated.\")\n",
        "    print(dataset)\n",
        "    print(dataset[0])\n",
        "    dataset.save_to_disk(dataset_name)\n",
        "    print(\"Dataset saved to disk.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCa8YZRKbcGl"
      },
      "outputs": [],
      "source": [
        "def push_to_hub():\n",
        "    hf_user = \"\"\n",
        "    hf_token = \"\"\n",
        "    dataset = Dataset.load_from_disk(name_for_dataset)\n",
        "    remote_hub_repo = hf_user + \"/\" + name_for_dataset\n",
        "    dataset.push_to_hub(remote_hub_repo, token=hf_token, private=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DDAzdGCbcGl"
      },
      "outputs": [],
      "source": [
        "  image_size_for_blip_inference = 384\n",
        "  setup(image_size_for_blip_inference)\n",
        "  process_image_folder(local_dataset_folder,name_for_dataset)\n",
        "  push_to_hub()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}