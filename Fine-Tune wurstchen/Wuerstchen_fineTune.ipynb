{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6363c4-28bf-42df-acaf-1c2578373598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (0.30.0)\n",
      "Requirement already satisfied: diffusers in /opt/conda/lib/python3.11/site-packages (0.28.0.dev0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.40.2)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.11/site-packages (0.10.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (2.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.11/site-packages (from diffusers) (6.8.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from diffusers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from diffusers) (2024.4.28)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from diffusers) (10.1.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2023.9.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata->diffusers) (3.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate diffusers transformers peft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af05c93-52d1-4c3e-8d7f-be6d8afdee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/diffusers\n",
      "  Cloning https://github.com/huggingface/diffusers to /tmp/pip-req-build-1nj4bcbo\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers /tmp/pip-req-build-1nj4bcbo\n",
      "  Resolved https://github.com/huggingface/diffusers to commit d50baf0c632342b9576a24352244c4235ce8b875\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (6.8.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.2 in /opt/conda/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (0.23.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (1.24.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (2024.4.28)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (0.4.3)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from diffusers==0.28.0.dev0) (10.1.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers==0.28.0.dev0) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers==0.28.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers==0.28.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers==0.28.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.2->diffusers==0.28.0.dev0) (4.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata->diffusers==0.28.0.dev0) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers==0.28.0.dev0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers==0.28.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers==0.28.0.dev0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->diffusers==0.28.0.dev0) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b239b15c-f86a-4880-90a2-b1dbbf1fbdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.11/site-packages (0.23.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub) (2023.7.22)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf02ef1af0fb40258122ded8eb92ea83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0655bc-bdb9-49d2-8667-6812b1c7056b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/sshety3/.netrc\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "!wandb login c8158c7f409290d812660cf23ccfce71c858f46a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915c233a-321f-445c-81b3-cd572ddb4e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "05/08/2024 03:56:00 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CLIPTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdlcvsp24\u001b[0m (\u001b[33mdlcv24\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/sshety3/wandb/run-20240508_035736-tqpp4kzp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdauntless-violet-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/dlcv24/text2image-fine-tune\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/dlcv24/text2image-fine-tune/runs/tqpp4kzp\u001b[0m\n",
      "05/08/2024 03:57:40 - INFO - __main__ - ***** Running training *****\n",
      "05/08/2024 03:57:40 - INFO - __main__ -   Num examples = 3141\n",
      "05/08/2024 03:57:40 - INFO - __main__ -   Num Epochs = 1\n",
      "05/08/2024 03:57:40 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "05/08/2024 03:57:40 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "05/08/2024 03:57:40 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "05/08/2024 03:57:40 - INFO - __main__ -   Total optimization steps = 350\n",
      "Steps: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 350/350 [01:15<00:00,  4.94it/s, lr=0.0001, step_loss=0.0191]05/08/2024 03:58:56 - INFO - __main__ - Running validation... \n",
      "{'prior_text_encoder', 'prior_tokenizer', 'prior_prior', 'prior_scheduler'} was not found in config. Values will be initialized to default values.\n",
      "Keyword arguments {'prior': WuerstchenPrior(\n",
      "  (projection): Conv2d(16, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (cond_mapper): Sequential(\n",
      "    (0): Linear(in_features=1280, out_features=1536, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2)\n",
      "    (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (2): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (4): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (5): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (6): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (7): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (8): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (9): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (10): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (11): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (12): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (13): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (14): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (15): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (16): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (17): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (18): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (19): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (20): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (21): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (22): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (23): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (24): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (25): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (26): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (27): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (28): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (29): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (30): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (31): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (32): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (33): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (34): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (35): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (36): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (37): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (38): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (39): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (40): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (41): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (42): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (43): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (44): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (45): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (46): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (47): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (48): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (49): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (50): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (51): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (52): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (53): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (54): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (55): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (56): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (57): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (58): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (59): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (60): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (61): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (62): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (63): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (64): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (65): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (66): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (67): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (68): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (69): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (70): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (71): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (72): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (73): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (74): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (75): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (76): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (77): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (78): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (79): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (80): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (81): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (82): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (83): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (84): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (85): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (86): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (87): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (88): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (89): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (90): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (91): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (92): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (93): ResBlock(\n",
      "      (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (channelwise): Sequential(\n",
      "        (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): GlobalResponseNorm()\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "        (4): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (94): TimestepBlock(\n",
      "      (mapper): Linear(in_features=64, out_features=3072, bias=True)\n",
      "    )\n",
      "    (95): AttnBlock(\n",
      "      (norm): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (attention): Attention(\n",
      "        (to_q): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_k): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_v): lora.Linear(\n",
      "          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Identity()\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (to_out): ModuleList(\n",
      "          (0): lora.Linear(\n",
      "            (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (default): Identity()\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (default): Linear(in_features=1536, out_features=4, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (default): Linear(in_features=4, out_features=1536, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "          )\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (kv_mapper): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): WuerstchenLayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "    (1): Conv2d(1536, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")} are not expected by WuerstchenCombinedPipeline and will be ignored.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of warp-ai/wuerstchen.\n",
      "\n",
      "Loading pipeline components...:  20%|â–ˆâ–ˆâ–Œ          | 1/5 [00:10<00:40, 10.25s/it]\u001b[ALoaded tokenizer as CLIPTokenizerFast from `tokenizer` subfolder of warp-ai/wuerstchen.\n",
      "\n",
      "Loading pipeline components...:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 2/5 [00:10<00:13,  4.36s/it]\u001b[ATraceback (most recent call last):\n",
      "  File \"/opt/conda/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n",
      "    args.func(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1082, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 688, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/opt/conda/bin/python3.11', 'train_text_to_image_lora_prior.py', '--mixed_precision=fp16', '--dataset_name=Norod78/cartoon-blip-captions', '--caption_column=text', '--resolution=32', '--train_batch_size=1', '--max_train_steps=350', '--checkpointing_steps=1000', '--learning_rate=1e-04', '--lr_scheduler=constant', '--lr_warmup_steps=0', '--seed=42', '--rank=4', '--report_to=wandb', '--validation_prompt=a man in a garden', '--push_to_hub', '--dataloader_num_workers=0', '--output_dir=wuerstchen-prior-cartoon-lora']' died with <Signals.SIGKILL: 9>.\n"
     ]
    }
   ],
   "source": [
    "!bash script.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72d6ad81-e755-40f2-bb99-2ae34b1c9f11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !bash script.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d184a5ac-afe9-4536-a67d-c916a5e4e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59849f15-9e96-4171-bb1f-5ffaa1c9a632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
